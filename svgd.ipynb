{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stein Variational Gradient Descent\n",
    "Step-by-step through a general SVGD implementation in `tensorflow 2`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the actual algorithm we need an update loop, a reasonable optimizer (e.g. Adam) and a couple of gradients. Also we need a kernel, but we will \n",
    "take of that afterwards. Let us assume the algorithm is already initialized with a kernel function that we can call with the current samples that \n",
    "returns a differentiable kernel matrix. In each iteration, the main method `update` first computes `kernelMatrix, kernelGrad` and `logprobGradient` and then \n",
    "simply computes the SVGD-gradient $\\frac{1}{N} (K \\nabla \\log p(x) - \\nabla K)$. Check out [this discussion](https://github.com/activatedgeek/svgd/issues/1)\n",
    "to see why the vectorized kernel gradient computation needs a **minus sign**.\n",
    "\n",
    "Also note how SVGD is defined for pertrubations which are **added** to the particles, not subtracted, as is customary for gradient descent and thus we counter the optimizers standard behaviour by another **minus sign** in front of the gradient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SVGD:\n",
    "    def __init__(self, kernel, targetDistribution, learningRate):\n",
    "        self.kernel = kernel\n",
    "        self.targetDistribution = targetDistribution\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learningRate)\n",
    "\n",
    "    def update(self, x, nIterations):\n",
    "        for _ in range(nIterations):\n",
    "            kernelMatrix, kernelGrad = self.computeKernel(x)\n",
    "            logprobGradient = self.logprobGradient(x)\n",
    "\n",
    "            # minus to cancel out the negative descent of Adam\n",
    "            completeGrad = -(kernelMatrix @ logprobGradient + kernelGrad) / x.shape[0]\n",
    "            self.optimizer.apply_gradients([(completeGrad, x)])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def computeKernel(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            kernelMatrix = self.kernel(x)\n",
    "\n",
    "        # why minus? see https://github.com/activatedgeek/svgd/issues/1 (right at the end)\n",
    "        return kernelMatrix, -tape.gradient(kernelMatrix, [x])[0]\n",
    "\n",
    "    def logprobGradient(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logprob = tf.math.log(self.targetDistribution(x))\n",
    "        return tape.gradient(logprob, [x])[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for the kernel we implement the RBF-kernel but any other proper kernel can be used as well. We start by defining the eculidean pairwise difference generalized to a matrix of particles (where particles can be in any $\\mathbb{R}^k$), which is the matrix version of \n",
    "\n",
    "$\\left\\lVert x - x' \\right\\rVert^2_2$.\n",
    "\n",
    "We stop the gradient of $x'$ since we want to take the gradient only w.r.t $x$:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "@tf.function\n",
    "def euclideanPairwiseDistance(x):\n",
    "    distance = tf.expand_dims(x, 1) - tf.expand_dims(tf.stop_gradient(x), 0)\n",
    "    return tf.einsum('ijk,kji->ij', distance, tf.transpose(distance))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use `tf.einsum` ([Einstein summation](https://www.tensorflow.org/api_docs/python/tf/einsum)) to do matrix computations in three dimensions.\n",
    "\n",
    "Now we define the RBF-kernel $\\exp -\\dfrac{1}{2h^2} \\left\\lVert x - x' \\right\\rVert^2_2$, while estimating bandwidth $h = \\text{med}^2 / \\log(N + 1)$, see [the SVGD paper, Section 5](https://arxiv.org/pdf/1608.04471.pdf). This allows for a dynamic adjustment according to the particles.\n",
    "\n",
    "We also stop the gradient for the bandwidth to improve the kernel gradient (we treat $h$ as a constant). Since `tensorflow` has no specific function to \n",
    "calculate the median, we use the one-liner from [this answer on SO](https://stackoverflow.com/a/47657076)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class RbfKernel:\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        normedDist = euclideanPairwiseDistance(x)\n",
    "        bandwidth = tf.stop_gradient(self.computeBandWidth(normedDist))\n",
    "        return tf.exp(-0.5 * normedDist / bandwidth**2)\n",
    "\n",
    "    @tf.function\n",
    "    def computeBandWidth(self, euclideanPwDistances):\n",
    "        pwDistanceMedian = tfp.stats.percentile(\n",
    "            euclideanPwDistances, 50.0, interpolation='midpoint')\n",
    "\n",
    "        n = tf.Scalar(euclideanPwDistances.shape[0])\n",
    "        return pwDistanceMedian / tf.math.log(n + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can find this implementation also in the repo and we will import it from there for the demo notebooks."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}